{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "import torch.optim as optim\n",
    "from torchtext import vocab\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field\n",
    "from torch.autograd import Variable\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import TabularDataset\n",
    "from torch.optim import adam\n",
    "device = torch.device(\"cuda:0\")\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training Data, Testing Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- LOAD_DATASET -------------\n",
    "    | Function  : load_dataset()\n",
    "    | Purpose   : Reads dataset(s) in CSV file format \n",
    "    | Arguments : \n",
    "    |       drive_path : Path to dataset file\n",
    "    |       dataset    : Dataset file name\n",
    "    | Return    :\n",
    "    |       dataset    : Dataset in dataframe format\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "def load_dataset(dataset):\n",
    "  loaded_dataset = pd.read_csv(dataset)     # Read CSV file\n",
    "  print(\"=\"*40, \"\\n\", loaded_dataset)                    # Print the dataset that we load in previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand and Pre-process Training Data, Testing Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- DATA_PRE-PROCESSING -------------\n",
    "    | Function  : data_pre-processing()\n",
    "    | Purpose   : Performs following pre-processing:\n",
    "    |              •\tRemove non-alphanumeric characters\n",
    "    |              •\tLower case\n",
    "    |              •\tRemove leading and trailing whitespaces\n",
    "    | Arguments : \n",
    "    |       text: Text to be pre-processed\n",
    "    | Return    :\n",
    "    |       text: Pre-processed text\n",
    "    *------------------------------------------------------------------------------------------------*/\n",
    "'''\n",
    "def data_pre_processing(text):\n",
    "      text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # Remove non alphanumeric character\n",
    "      text = text.lower()                        # Lowercase all text\n",
    "      return text.strip()                        # Remove leading and trailing whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- TOKENIZE_TEXT -------------\n",
    "    | Function  : data_tokenization()\n",
    "    | Purpose   : Tokenizes a Text\n",
    "    | Arguments : \n",
    "    |       text: Text to be tokenized\n",
    "    | Return    :\n",
    "    |       text: Tokenized Text\n",
    "    *------------------------------------------------------------------------------------------------*/\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def data_tokenization(s):\n",
    "    return word_tokenize(s.lower())        # Apply pre-processing function (created in previous step) and tokenizer on text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Training Data, Testing Data and Validation Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- BUILD_DATA_OBJECTS -------------\n",
    "    | Function  : data_objects()\n",
    "    | Purpose   : Build pre-processed and tokenized data objects\n",
    "    | Arguments : \n",
    "    |       drive_path : Path of the directory where data files are placed\n",
    "    | Return    :\n",
    "    |       pre_processed_training_data, pre_processed_validation_data, pre_processed_testing_data, LABEL, TEXT\n",
    "    *------------------------------------------------------------------------------------------------*/\n",
    "'''\n",
    "def data_objects():\n",
    "  # Declared a Field object \n",
    "  # Field : A class that stores information about the way of preprocessing\n",
    "  TEXT  = Field(sequential= True, tokenize = data_tokenization, lower = True, include_lengths = False, batch_first = False, init_token = '<sos>', \n",
    "            eos_token = '<eos>')\n",
    "  LABEL = data.LabelField(dtype = torch.float)\n",
    "  # TabularDataset : Defines a dataset of columns. Create a TabularDataset given a path, file format, \n",
    "  # and Field list\n",
    "  training_data, validation_data, testing_data = TabularDataset.splits(path = '../Dataset/',\n",
    "                                                            train       = 'Train_Data.csv', \n",
    "                                                            validation  = 'Validation_Data.csv',\n",
    "                                                            test        = 'Test_Data.csv',\n",
    "                                                            format      = 'csv', \n",
    "                                                            fields      = [('text', TEXT),('haveBugs', LABEL),('invalidPositionOverTime', LABEL),('implementationResponseIssue', LABEL),('invalidContextOverTime', LABEL),\n",
    "                                                            ('interruptedEvent', LABEL),('invalidEventOccurraceOverTime', LABEL),('actionNotPossible', LABEL),('actionWhenNotAllowed', LABEL),('informationOutOfOrder', LABEL),\n",
    "                                                            ('lackOfRequiredInformation', LABEL),('invalidInfoAccess', LABEL),('objectOutOfBoundForAnyState', LABEL),('objectOutOfBoundForSpecificState',LABEL),\n",
    "                                                            ('artificialStupidity',LABEL), ('invalidValueChange',LABEL),('invalidGraphicalRespresentation',LABEL) ], \n",
    "                                                            skip_header = True)\n",
    "  print(\"\\nPre-processed and Tokenized Training Data:\")\n",
    "  print(\"\\n=========================================\")\n",
    "  for i in range(len(training_data)):\n",
    "    print(training_data[i].text)\n",
    "  print(\"\\nPre-processed and Tokenized Validation Data:\")\n",
    "  print(\"\\n=========================================\")\n",
    "  for i in range(len(validation_data)):\n",
    "    print(validation_data[i].text)\n",
    "  print(\"\\nPre-processed and Tokenized Testing Data:\")\n",
    "  print(\"\\n=========================================\")\n",
    "  for i in range(len(testing_data)):\n",
    "      print(testing_data[i].text)\n",
    "  \n",
    "  return training_data, validation_data, testing_data, LABEL, TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-Trained Word Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- LOAD_WORD_EMBEDDING_VECTORS -------------\n",
    "    | Function  : load_word_embedding_vectors()\n",
    "    | Purpose   : Load pre-trained word embedding vectors from memory\n",
    "    | Arguments : \n",
    "    |       drive_path : Path to word embedding vectors file\n",
    "    | Return    :\n",
    "    |       vectors     : Loaded word embedding vectors\n",
    "    *------------------------------------------------------------------------------------------------*/\n",
    "'''\n",
    "def load_word_embedding_vectors():\n",
    "  # Load word embedding vectors from memory \n",
    "  # I have downloaded the Glove word embedding vectors 100d from internet and saved in my drive \n",
    "  # To use that, I simply give the path of that file and read file in my program using vocab.Vectors function\n",
    "  vectors = vocab.Vectors('glove.6B.100d.txt', '../kaggle/')\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- BUILD_VOCABULARY -------------\n",
    "    | Function  : build_vocabulary()\n",
    "    | Purpose   : Build vocabulary from input data\n",
    "    | Arguments : \n",
    "    |       pre_processed_training_data   : Pre-processed training data\n",
    "    |       pre_processed_validation_data : Pre-processed validation data\n",
    "    |       pre_processed_testing_data    : Pre-processed testing data\n",
    "    |       vectors                       : Word embedding vectors \n",
    "    |       LABEL                         : LABEL object (Pre-processing applied on output)\n",
    "    |       TEXT                          : TEXT object (Pre-processing applied on input)\n",
    "    | Return    :\n",
    "    |       word_embeddings               : Word embedding vectors mapped on data\n",
    "    |       vocabulary_size               : Size of vocabulary\n",
    "    *------------------------------------------------------------------------------------------------*/\n",
    "'''\n",
    "\n",
    "def build_vocabulary(training_data, validation_data , testing_data, vectors, LABEL, TEXT):\n",
    "  TEXT.build_vocab(training_data, validation_data , testing_data, vectors=vectors, unk_init=torch.Tensor.normal_)   # Build vocabulary from input text\n",
    "  LABEL.build_vocab(training_data, validation_data , testing_data)                   # Build vocabulary from output / labels (Encode all labels)\n",
    "  \n",
    "  print(\"\\n=========================================\")\n",
    "  print(\"Output/Label word to index dictionary: \", LABEL.vocab.stoi)\n",
    "  print(\"\\n=========================================\")\n",
    "  print(\"Input Text word to index dictionary:\\n \", TEXT.vocab.stoi,\"\\n\")\n",
    "  \n",
    "  word_embeddings = TEXT.vocab.vectors   # Load vectors\n",
    "  vocabulary_size = len(TEXT.vocab)      # Size of vocabulary\n",
    "  return word_embeddings, vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent Training Data, Testing Data and Validation Data in Machine Understandable Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- REPRESENT_DATA_IN_MACHINE_UNDERSTANDABLE_FORMAT -------------\n",
    "    | Function  : data_iterators()\n",
    "    | Purpose   : To build input data (Training, validation and testing data) iterators \n",
    "    |             (It will convert data into machine understandable format and make data objects which we can iterate over during model training and testing)\n",
    "    | Arguments : \n",
    "    |       pre_processed_training_data   : Pre-processed training data\n",
    "    |       pre_processed_validation_data : Pre-processed validation data\n",
    "    |       pre_processed_testing_data    : Pre-processed testing data\n",
    "    | Return    :\n",
    "    |       training_iterator   : Training data iterator object\n",
    "    |       validation_iterator : Validation data iterator object\n",
    "    |       testing_iterator    : Testing data iterator object\n",
    "    *------------------------------------------------------------------------------------------------*/\n",
    "'''\n",
    "\n",
    "def data_iterators(training_data, validation_data, testing_data):\n",
    "  # Iterators handle numericalizing, batching, packaging. Basically, it does all the heavy lifting necessary \n",
    "  # to pass the data to a neural network\n",
    "  # BucketIterator : Defines an iterator that batches examples of similar lengths together to minimizes the amount of padding needed\n",
    "  # By using \"splits\" it applies processing steps on all datasets equally\n",
    "\n",
    "  training_iterator, validation_iterator, testing_iterator = data.BucketIterator.splits((training_data, validation_data, testing_data), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "  print(\"\\nTraining Data Tensors Form\\n\")\n",
    "  print(\"=\"*30, \"\\n\")\n",
    "  for batch in training_iterator:\n",
    "    print(batch.text)\n",
    "  print(\"\\nValidation Data Tensors Form\\n\")\n",
    "  print(\"=\"*30, \"\\n\")\n",
    "  for batch in validation_iterator:\n",
    "    print(batch.text)\n",
    "  print(\"\\nTesting Data Tensors Form\\n\")\n",
    "  print(\"=\"*30, \"\\n\")\n",
    "  for batch in testing_iterator:\n",
    "    print(batch.text)\n",
    "  return training_iterator, validation_iterator, testing_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+============================Data Preparation============================+\\n\\n\")\n",
    "print(\"---Step 2: Load Training Data, Testing Data and Validation Data---\")\n",
    "print(\"\\nTraining data before pre_processing\")\n",
    "original_training_data = load_dataset(\"../Dataset/Train_Data.csv\")\n",
    "\n",
    "print(\"\\n\\nValidation data before pre_processing\")\n",
    "original_validation_data = load_dataset(\"../Dataset/Validation_Data.csv\")\n",
    "\n",
    "print(\"\\n\\nTesting data before pre_processing\")\n",
    "original_testing_data = load_dataset(\"../Dataset/Test_Data.csv\")\n",
    "print(\"\\n---Step 3: Understand and Pre-process Training Data, Testing Data and Validation Data---\")\n",
    "print(\"\\n---Step 4: Represent Training Data, Testing Data and Validation Data in Machine Understandable Format---\")\n",
    "preprocessed_training_data, preprocessed_validation_data, preprocessed_testing_data, LABEL, TEXT = data_objects()\n",
    "# Load word embedding vectors from memory\n",
    "vectors = load_word_embedding_vectors()\n",
    "# Build vocabulary\n",
    "word_embeddings, vocabulary_size = build_vocabulary(preprocessed_training_data, preprocessed_validation_data, preprocessed_testing_data, vectors, LABEL, TEXT)\n",
    "# Create iterator objects\n",
    "training_iterator, validation_iterator, testing_iterator = data_iterators(preprocessed_training_data, preprocessed_validation_data, preprocessed_testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Training Phase:\n",
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- MODEL_ARCHITECTURE -------------\n",
    "    | Class     : RNN()\n",
    "    | Purpose   : To build the architecture of model to be trained\n",
    "    *---------------------------------------------------------\n",
    "    | nn.Module : Base class for all neural network modules. Your models should also subclass this class.\n",
    "    |\n",
    "    | Arguments:\n",
    "    |      output_dim    : For output layer number of nodes in output layer will be same as \n",
    "    |                      number of outputs required in your problem\n",
    "    |\t     hidden_dim    : Size of the hidden layer. Here size of hidden_state of the RNN\n",
    "    | \t\t input_dim     : Size of the vocabulary containing unique words. Total number of unique words in sample data \n",
    "    |\t\t   embedding_dim : Size of each embedding vector. Here embeddding dimension of GloVe word embedding \n",
    "    |                      vectors is 100 so embedding_dim = 100\n",
    "    |\t\t   weights       : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
    "    *------------------------------------------------------------------------------------\n",
    "    | Function  : forward()\n",
    "    | Purpose   : This function will automatically start foward propogation when model object is called\n",
    "    | Arguments :\n",
    "    |     text  : Input text of shape = (num_sequences, batch_size)\t\n",
    "\t  | Return:\n",
    "\t  |     hidden_state : Final model state learned from input text\n",
    "    ------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, word_embeddings):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embedding_dim)          # Embedding layer shape\n",
    "        # Assign pre-trained weights and train during model training \n",
    "        # So the weight will be updated during backpropogation(if you don't want to train them during model training set requires_grad = False))\n",
    "        self.embedding_layer.weight = nn.Parameter(word_embeddings, requires_grad=True)      \n",
    "        self.rnn_layer       = nn.RNN(embedding_dim, hidden_dim, num_layers=1) # We can implement multiple layers of RNN simply by changing num_layers value \n",
    "        self.linear_layer    = nn.Linear(hidden_dim, output_dim)               # Shape of linear layer\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        h_0 = self.init_hidden()   # Initialize first hidden state to all zeros\n",
    "        \n",
    "        # Here we will map all the indexes present in the input sequence to the corresponding \n",
    "\t\t    # word vector using our trained word_embedddings.\n",
    "\t      # embedded input of shape = (num_sequences, batch_size, embedding_dimension)\n",
    "        embedded_vectors = self.embedding_layer(text)  \n",
    "        print (embedded_vectors)                  \n",
    "        output_state, hidden_state = self.rnn_layer(embedded_vectors, h_0)  # Apply RNN layer and start learning sequence of words\n",
    "        hidden_state = self.linear_layer(hidden_state.squeeze(0))      # Apply the linear layer on output\n",
    "        return torch.sigmoid(hidden_state)\n",
    "    def init_hidden(self):                                             # init_hidden for each beginning of sentence\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        return h_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "/*---------------- INITIALIZE_PARAMETERS ------------------\n",
    "'''\n",
    "input_dimension     = len(TEXT.vocab)\n",
    "embedding_dimension = 100\n",
    "hidden_dimension    = 10\n",
    "output_dimension    = 16\n",
    "number_of_epochs    = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/* ----------------------- MODEL_OBJECT -----------------\n",
    "| Create the object of model class and pass parameters required: RNN()\n",
    "|           Arguments : \n",
    "|               input_dimension     : (integer) dimension of input layer(vocabulary size)\n",
    "|               output_dimension    : (integer) number of output layer nodes \n",
    "|               hidden_dimension    : (integer) number of nodes/units in hidden layer\n",
    "|               embedding_dimension : (integer) dimension of embedded vector\n",
    "*-------------------------------------------------------*/\n",
    "\"\"\"\n",
    "model = RNN(input_dimension, embedding_dimension, hidden_dimension, output_dimension, word_embeddings)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)   # Initialize the optimizer\n",
    "criterion = nn.BCELoss()                     # Intialize loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score           # Import multi-label Accuracy (or Jaccard index)\n",
    "\n",
    "''' \n",
    "    /*----------------------------- CALCULATE_MULTI-LABEL_ACCURACY -------------\n",
    "    | Function  : calculate_jaccard()\n",
    "    | Purpose   : Calculate Multi-label Accuracy Score\n",
    "    | Arguments : \n",
    "    |       prediction : Predicted values\n",
    "    |       label      : Actual values\n",
    "    | Return    :\n",
    "    |       jaccard   : Multi-label Accuracy score\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "def calculate_jaccard(prediction, label):              # Calculate multi-label Accuracy (or Jaccard index)\n",
    "\n",
    "    rounded_preds = torch.round(prediction)       \n",
    "    true = label.detach().cpu().numpy().reshape(-1)\n",
    "    pred = rounded_preds.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    jaccard=torch.tensor(jaccard_score(true,pred))\n",
    "\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score                   # Import f1_score\n",
    "'''\n",
    "    /*----------------------------- CALCULATE_F1_SCORE -------------\n",
    "    | Function  : calculate_f1()\n",
    "    | Purpose   : Calculate F1 Score\n",
    "    | Arguments : \n",
    "    |       prediction : Predicted values\n",
    "    |       label      : Actual values\n",
    "    | Return    :\n",
    "    |       f1_score(true,pred)   : F1 score\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "def calculate_f1(prediction, label):                   # Calculate f1_score\n",
    "\n",
    "    rounded_preds = torch.round(prediction)       \n",
    "    true = label.detach().cpu().numpy().reshape(-1)\n",
    "    pred = rounded_preds.detach().cpu().numpy().reshape(-1)\n",
    "    return f1_score(true,pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss               # Import hamming_loss\n",
    "'''\n",
    "    /*----------------------------- CALCULATE_HAMMING_LOSS_SCORE -------------\n",
    "    | Function  : calculate_hl()\n",
    "    | Purpose   : Calculate Hamming Loss Score\n",
    "    | Arguments : \n",
    "    |       prediction : Predicted values\n",
    "    |       label      : Actual values\n",
    "    | Return    :\n",
    "    |       hamming_loss(true,pred)   : Hamming Loss Score\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "def calculate_hl(prediction, label):                   # Calculate hamming_loss\n",
    "\n",
    "    rounded_preds = torch.round(prediction)       \n",
    "    true = label.detach().cpu().numpy().reshape(-1)\n",
    "    pred = rounded_preds.detach().cpu().numpy().reshape(-1)\n",
    "    return hamming_loss(true,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    accuracy =torch.mean(torch.div(torch.sum(torch.eq(y_true, torch.round(y_pred)), dim=-1),torch.tensor(y_true.shape[1],dtype=float)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Epoch Elapsed Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- EPOCH_TIME_CALCULATION -------------\n",
    "    | Function  : epoch_time()\n",
    "    | Purpose   : Calculate time elapsed in each epoch\n",
    "    | Arguments : \n",
    "    |        start_time   : Time when an epoch's execution starts\n",
    "    |        end_time     : Time when an epoch's execution end\n",
    "    | Return    :\n",
    "    |        elapsed_mins : Time consumed by one epoch in minutes\n",
    "    |        elapsed_secs : Time consumed by one epoch in seconds\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time                   # Time elapsed by one epoch \n",
    "    elapsed_mins = int(elapsed_time / 60)                  # Convert time in minutes\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) # Convert time in seconds\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss      = 0                                                 # Initialize epoch loss to 0\n",
    "epoch_accuracy  = 0                                                 # Initialize epoch accuracy to 0\n",
    "\n",
    "model.train()         \n",
    "# Start model training mode\n",
    "\n",
    "for batch in training_iterator:\n",
    "    optimizer.zero_grad()                                           # Clear all optimized gradients\n",
    "    predictions = model(batch.text).squeeze(1)                     # Make model predictions on training data\n",
    "    y_true=torch.stack((batch.haveBugs,batch.invalidPositionOverTime,batch.implementationResponseIssue,batch.invalidContextOverTime,batch.interruptedEvent,\n",
    "    batch.invalidEventOccurraceOverTime,batch.actionNotPossible,batch.actionWhenNotAllowed,batch.informationOutOfOrder,batch.lackOfRequiredInformation,batch.invalidInfoAccess,batch.objectOutOfBoundForAnyState,\n",
    "    batch.objectOutOfBoundForSpecificState,batch.artificialStupidity,batch.invalidValueChange,batch.invalidGraphicalRespresentation\n",
    "    ),dim=1)\n",
    "\n",
    "    loss     = criterion(predictions, y_true)                 # Calculate loss for each batch in epoch\n",
    "    accuracy = calculate_accuracy(predictions, y_true) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- TRAIN_MODEL -------------\n",
    "    | Function  : train()\n",
    "    | Purpose   : Train Model\n",
    "    | Arguments : \n",
    "    |        model                 : Model object\n",
    "    |        training_data_iterator: Training data iterator object\n",
    "    |        optimizer             : Optimization algorithm\n",
    "    |        criterion             : Loss funtion\n",
    "    | Return    :\n",
    "    |        epoch_loss            : Train data loss at each epoch\n",
    "    |        epoch_accuracy        : Train data accuracy at each epoch\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss      = 0                                                 # Initialize epoch loss to 0\n",
    "    epoch_accuracy  = 0                                                 # Initialize epoch accuracy to 0\n",
    "    epoch_hamming=0\n",
    "    epoch_f1=0\n",
    "    model.train()                                                       # Start model training mode\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()                                           # Clear all optimized gradients\n",
    "        predictions = model(batch.text).squeeze(1)                     # Make model predictions on training data\n",
    "        y_true=torch.stack((batch.haveBugs,batch.invalidPositionOverTime,batch.implementationResponseIssue,batch.invalidContextOverTime,batch.interruptedEvent,\n",
    "    batch.invalidEventOccurraceOverTime,batch.actionNotPossible,batch.actionWhenNotAllowed,batch.informationOutOfOrder,batch.lackOfRequiredInformation,batch.invalidInfoAccess,batch.objectOutOfBoundForAnyState,\n",
    "    batch.objectOutOfBoundForSpecificState,batch.artificialStupidity,batch.invalidValueChange,batch.invalidGraphicalRespresentation,),dim=1)\n",
    "\n",
    "        loss     = criterion(predictions, y_true)                 # Calculate loss for each batch in epoch\n",
    "        accuracy = calculate_accuracy(predictions, y_true)        # Calculate accuracy for each batch in epoch\n",
    "        hamming=calculate_hl(predictions, y_true)                 # Calculate hamming for each batch in epoch\n",
    "        f1=calculate_f1(predictions, y_true)                      # Calculate f1 for each batch in epoch\n",
    "        \n",
    "        loss.backward()                                                  # Start backward propogation\n",
    "        optimizer.step()                                                 # Optimization of parameters\n",
    "        \n",
    "        epoch_loss      += loss.item()                                   # Add loss for all batches in one epoch\n",
    "        epoch_accuracy  += accuracy.item()    \n",
    "        epoch_hamming+=hamming.item()\n",
    "        epoch_f1 +=f1.item()\n",
    "        # Add accuracy for all batches in one epoch\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator), epoch_hamming / len(iterator),epoch_f1 / len(iterator)   # Average loss, accuracy, hamming, and f1 for one epoch and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- SAVE_MODEL -------------\n",
    "    | Function  : save_model()\n",
    "    | Purpose   : Save a trained model on your hard disk\n",
    "    | Arguments : \n",
    "    |        drive_path: Path to the directory where the trained model will be saved\n",
    "    | Return    :\n",
    "    |        Trained model will be saved on hard disk\n",
    "    *---------------------------------------------------------*/\n",
    "\n",
    "'''\n",
    "def save_model():\n",
    "  torch.save(model.state_dict(), '../Model/' + 'best-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- Evaluate_MODEL -------------\n",
    "    | Function  : evaluate()\n",
    "    | Purpose   : Function to be used in Validation and Test Phase\n",
    "    | Arguments : \n",
    "    |        model                : Model object\n",
    "    |        data_iterator:  Data iterator object\n",
    "    | Return    :\n",
    "    |        epoch_loss           : Data loss at each epoch\n",
    "    |        epoch_accuracy       : Data accuracy at each epoch\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_f1       =0       # Initialize epoch f1 to 0\n",
    "    epoch_loss     = 0      # Initialize epoch loss to 0\n",
    "    epoch_accuracy = 0      # Initialize epoch accuracy to 0\n",
    "    epoch_hamming  =0.      # Initialize epoch hamming to 0\n",
    "    model.eval()            # Start model evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            y_true=torch.stack((batch.haveBugs,batch.invalidPositionOverTime,batch.implementationResponseIssue,batch.invalidContextOverTime,batch.interruptedEvent,\n",
    "    batch.invalidEventOccurraceOverTime,batch.actionNotPossible,batch.actionWhenNotAllowed,batch.informationOutOfOrder,batch.lackOfRequiredInformation,batch.invalidInfoAccess,batch.objectOutOfBoundForAnyState,\n",
    "    batch.objectOutOfBoundForSpecificState,batch.artificialStupidity,batch.invalidValueChange,batch.invalidGraphicalRespresentation),dim=1)\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1) \n",
    " \n",
    "                # Make model predictions on data\n",
    "            loss     = criterion(predictions, y_true)                 # Calculate loss for each batch in epoch\n",
    "            accuracy = calculate_accuracy(predictions, y_true)        # Calculate accuracy for each batch in epoch\n",
    "            f1 = calculate_f1(predictions, y_true)                    # Calculate f1 for each batch in epoch\n",
    "            hamming=calculate_hl(predictions, y_true)                 # Calculate hamming for each batch in epoch\n",
    "            epoch_loss += loss.item()                                 # Add loss for all batches, in one epoch\n",
    "            epoch_accuracy += accuracy.item()                         # Add accuracy for all batches in one epoch\n",
    "            epoch_f1+=f1.item()                                       # Add f1 for all batches in one epoch\n",
    "            epoch_hamming=hamming.item()                              # Add hamming for all batches in one epoch\n",
    "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator), epoch_hamming / len(iterator),epoch_f1 / len(iterator)  # Average loss, accuracy, hamming, and f1 for one epoch and return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- VALIDATE_MODEL -------------\n",
    "    | Function  : validation()\n",
    "    | Purpose   : Evalaute the performance of a trained  model\n",
    "    | Arguments : \n",
    "    |        model                   : Model object\n",
    "    |        validation_data_iterator: Validation data iterator object\n",
    "    |        criterion               : Loss function\n",
    "    | Return    :\n",
    "    |        epoch_loss           : Validation data loss at each epoch\n",
    "    |        epoch_accuracy       : Validation data accuracy at each epoch\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "\n",
    "def validation(model, validation_iterator, criterion):\n",
    "      best_validation_loss = float('inf')                                                        # Declare best validation loss variable\n",
    "      validation_loss, validation_accuracy,ham,f1 = evaluate(model, validation_iterator, criterion)     # Start model validation phase\n",
    "      \n",
    "      if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        save_model()                                   # Save model on epoch where the validation loss is lowest\n",
    "      return validation_loss, validation_accuracy,ham,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/*---------------------- LOAD_SAVED_MODEL ----------\n",
    "|  Function  : load_model()\n",
    "|  Purpose   : Method to load previously saved model\n",
    "|  Arguments :\n",
    "|       drive_path : Path of directory where model is saved\n",
    "|  Return    :\n",
    "|              Saved model will be loaded in memory\n",
    "*---------------------------------------------------------*/\n",
    "\"\"\"\n",
    "def load_model():\n",
    "  return model.load_state_dict(torch.load('../Model/'+ 'best-model.pt'))  # Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- TEST_MODEL -------------\n",
    "    | Function  : test()\n",
    "    | Purpose   : Evalaute the performance of a trained  model\n",
    "    | Arguments : \n",
    "    |        model                : Model object\n",
    "    |        testing_data_iterator: Test data iterator object\n",
    "    |        criterion            : Loss function\n",
    "    | Return    :\n",
    "    |        epoch_loss           : Test data loss at each epoch\n",
    "    |        epoch_accuracy       : Test data accuracy at each epoch\n",
    "    *---------------------------------------------------------*/\n",
    "'''\n",
    "\n",
    "def test(model, testing_iterator, criterion):\n",
    "  load_model()\n",
    "  testing_loss, testing_accuracy,ham,f1 = evaluate(model, testing_iterator, criterion)   # Start model testing\n",
    "  return testing_loss, testing_accuracy,ham,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Input from User and Convert it into Feature Vector Same as Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Application Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    /*----------------------------- USER_INPUT -------------\n",
    "    | Function  : take_user_input()\n",
    "    | Purpose   : Take unseen input from user\n",
    "    | Arguments : \n",
    "    |        TEXT : Field object to apply pre-processing on input text (same as sample data)\n",
    "    | Return    :\n",
    "    |        user_comment_tensor : User input in machine understandable format\n",
    "    |----------------------------------------------------------\n",
    "    | - Let us now predict the emotion on a single SMS for the real time evaluation purpose \n",
    "    | 1 : Take input from user\n",
    "    | 2 : Preprocess the user input\n",
    "    | 3 : Fit vocabulary previously made for sample data on user input. The indexes assigned for words in \n",
    "    |     sample data will be assigned to user input. Words in user input that does not appear in \n",
    "    |     sample data will have zero value\n",
    "    | 4 : Convert user input to an array\n",
    "    | 5 : Make tensor from array. As pytorch only work with tensors\n",
    "    *---------------------------------------------------------*/\n",
    "\n",
    "'''\n",
    "\n",
    "def take_user_input(TEXT):\n",
    "  user_comment = input(\"Enter comment: \") \n",
    "  \n",
    "  #Preprocess user input\n",
    "  preprocessed_user_comment = TEXT.preprocess(user_comment)\n",
    "  preprocessed_user_comment = [TEXT.init_token] + preprocessed_user_comment + [TEXT.eos_token]\n",
    "  user_comment_vocabulary = [TEXT.vocab.stoi[x] for x in preprocessed_user_comment]\n",
    "  user_comment_array = np.asarray(user_comment_vocabulary)\n",
    "  user_comment_tensor = torch.LongTensor(user_comment_array).unsqueeze(1)\n",
    "  user_comment_tensor = user_comment_tensor\n",
    "\n",
    "  print(\"\\nPreprocessed User_input:\\n==========================\")\n",
    "  print(preprocessed_user_comment)\n",
    "  print(\"\\nIdx stored in vocab, corresponding to each word in user_input:\\n==========================\")\n",
    "  print(user_comment_vocabulary) \n",
    "  print(\"\\nUser_input as a tensor:\\n==========================\" )\n",
    "  print(user_comment_tensor)\n",
    "  \n",
    "  return user_comment_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/*----------------------- MODEL_PREDICTION --------\n",
    "|  Function  : model_prediction()\n",
    "|  Purpose   : Use trained model to predict the output of unseen instances\n",
    "|  Arguments : \n",
    "|       user_input : Input taken from user\n",
    "|       drive_path : Path of the directory where trained model is saved\n",
    "|  Return    : \n",
    "|       out     : Set of Emotion labels that best represent the mental state of a writer \n",
    "*--------------------------------------------------\n",
    "|   1. Set the model to evaluation mode\n",
    "|   2. Set all the gradients to zero\n",
    "|   3. Apply trained model on user input\n",
    "|   \n",
    "|   4. torch.round() : \n",
    "|         Return the value rounded to the closest integer (0 or 1)\n",
    "*-------------------------------------------------*/   \n",
    "\"\"\"\n",
    "\n",
    "def model_predictions(user_input):\n",
    "  # Evaluate model\n",
    "  load_model()  # Load model from memory to test its performance\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    #print(Evaluate_text_tensor)\n",
    "    # Model Prediction\n",
    "    out = model(user_input)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n+=====================Execute the Training and Validation Phase=====================+\\n\\n\")\n",
    "# Step 5: Execute the Training Phase\n",
    "import time\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    start_time = time.time()                                    # Start time when one epoch will start executing\n",
    "    \n",
    "    training_loss, training_accuracy,train_ham,train_f1 = train(model, training_iterator, optimizer, criterion)   # Start model training\n",
    "    \n",
    "    # Step 7: Execute the Validation Phase\n",
    "    validation_loss, validation_accuracy,val_ham,val_f1 = validation(model, validation_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()                                       # End time when one epoch will end executing\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)    # Calculate time consumed by one epoch (in minutes and seconds)\n",
    "      \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTraining Loss: {training_loss:.3f}   | Training Accuracy: {training_accuracy*100:.2f} | Training Hamming: {train_ham*100:.2f}| Training F1: {train_f1*100:.2f}%')\n",
    "    print(f'\\tValidation Loss: {validation_loss:.3f} |  Validation Accuracy: {validation_accuracy*100:.2f}| Val Hamming: {val_ham*100:.2f}| Val F1: {val_f1*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n+=====================Execute the Testing Phase=====================+\\n\\n\")\n",
    "# Step 8: Execute the Testing Phase\n",
    "\n",
    "testing_loss, testing_accuracy,test_ham,test_f1 = test(model, testing_iterator, criterion)\n",
    "print(f'Testing Loss: {testing_loss:.3f} | Testing Accuracy: {testing_accuracy*100:.2f}| test Hamming: {val_ham*100:.2f}| test F1: {val_f1*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n+===================Execute the Application Phase===================+\\n\\n\")\n",
    "\n",
    "# Step 7: Execute the Application Phase\n",
    "\n",
    "user_input = take_user_input(TEXT)   # Take unseen input from user\n",
    "Label = model_predictions(user_input)  # Make trained model predictions on user input\n",
    "\n",
    "\n",
    "label_list=['haveBugs','invalidPositionOverTime','implementationResponseIssue','invalidContextOverTime','interruptedEvent','invalidEventOccurraceOverTime','actionNotPossible','actionWhenNotAllowed','informationOutOfOrder','lackOfRequiredInformation','invalidInfoAccess','objectOutOfBoundForAnyState','objectOutOfBoundForSpecificState','artificialStupidity','invalidValueChange','invalidGraphicalRespresentation']\n",
    "print(\"\\n+===================Trained Model Prediction===================+\\n\\n\")\n",
    "# print(label_list[np.argmax(Label.cpu().numpy(),axis=1)[0]])\n",
    "print([i for i,j in zip(label_list,list(Label.cpu().numpy()[0])) if j>0.50]) # Threshold for the probability (scoring into a class label)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29040de7e6f5de5d2bd1b40f6cd95c54deb9d87fe13eb19a8903168d595b640b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
